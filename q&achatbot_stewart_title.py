# -*- coding: utf-8 -*-
"""Q&Achatbot_Stewart_Title.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QttB6ed4qL2JoZbilxExlx7_AAiE_-cO
"""

!pip3 install sentence-transformers chromadb pandas numpy
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install -q chromadb sentence-transformers pandas openpyxl

from google.colab import files

uploaded = files.upload()

from google.colab import files

uploaded = files.upload()

import pandas as pd
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

import os

df1 = pd.read_excel("data.xlsx")
df2 = pd.read_excel("Forcast.xlsx")

def dataframe_to_text_chunks(df, table_name: str, max_chunk_chars: int = 1000) -> list:
    """
    Converts DataFrame rows into text chunks for embedding.
    Splits long rows into multiple smaller chunks to avoid token overflows.
    """
    chunks = []
    for _, row in df.iterrows():
        parts = [f"{col}: {str(row[col])}" for col in df.columns if pd.notnull(row[col])]
        full_text = f"[{table_name}] " + "; ".join(parts)

        if len(full_text) <= max_chunk_chars:
            chunks.append(full_text)
        else:
            current = f"[{table_name}] "
            for part in parts:
                if len(current) + len(part) + 2 <= max_chunk_chars:
                    current += part + "; "
                else:
                    chunks.append(current.strip("; "))
                    current = f"[{table_name}] {part}; "
            if current:
                chunks.append(current.strip("; "))
    return chunks

chunks1 = dataframe_to_text_chunks(df1, "Data")
chunks2 = dataframe_to_text_chunks(df2, "Forcast")
all_chunks = chunks1 + chunks2

print(f"âœ… Total optimized chunks: {len(all_chunks)}")

import torch
from sentence_transformers import SentenceTransformer

# Detect GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸ” Using device: {device}")

# Load model on GPU if available
model = SentenceTransformer("all-MiniLM-L6-v2", device=device)

# Embedding step
print("ðŸ”„ Embedding...")
embeddings = model.encode(
    all_chunks,
    convert_to_numpy=True,
    show_progress_bar=True,
    device=device  # This ensures the actual computation is on GPU
)
print("âœ… Embedding completed.")

import shutil
import os

chroma_path = "/content/chroma_rag_db"
if os.path.exists(chroma_path):
    shutil.rmtree(chroma_path)
os.makedirs(chroma_path, exist_ok=True)

import chromadb
from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE

client = chromadb.PersistentClient(
    path=chroma_path,
    settings=Settings(),
    tenant=DEFAULT_TENANT,
    database=DEFAULT_DATABASE,
)

collection = client.get_or_create_collection(name="excel_chunks")

print("ðŸ’¾ Saving to ChromaDB...")
batch_size = 5000
for i in range(0, len(all_chunks), batch_size):
    end = min(i + batch_size, len(all_chunks))
    collection.add(
        documents=all_chunks[i:end],
        embeddings=embeddings[i:end],
        ids=[f"doc_{j}" for j in range(i, end)]
    )

print(f"âœ… ChromaDB saved at: {chroma_path}")

!zip -r chroma_rag_db.zip chroma_rag_db
files.download("chroma_rag_db.zip")

